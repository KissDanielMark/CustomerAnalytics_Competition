{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Kiss Dániel Márk</center>\n",
    "## <center>WP871Q</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verseny_public_train = pd.read_csv('data/verseny_public_train.csv', sep=',', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data familirsation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_verseny_public_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_verseny_public_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_verseny_public_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df_verseny_public_train.drop(['target', 'cookie_id'], axis=1)\n",
    "y = df_verseny_public_train['target']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "X_pca\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "\n",
    "plt.xlabel('First principal component')\n",
    "\n",
    "plt.ylabel('Second principal component')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X = df_verseny_public_train.drop(['target', 'cookie_id'], axis=1)\n",
    "y = df_verseny_public_train['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. Feature %d (%f) %s\" % (f + 1, indices[f], importances[indices[f]], X.columns[indices[f]]))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"clf_rf = RandomForestClassifier()\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "importances_rf = clf_rf.feature_importances_\n",
    "\n",
    "\n",
    "indices_rf = np.argsort(importances_rf)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. Feature %d (%f) %s\" % (f + 1, indices_rf[f], importances_rf[indices_rf[f]], X.columns[indices_rf[f]]))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances (Random Forest)\")\n",
    "plt.bar(range(X.shape[1]), importances_rf[indices_rf],\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices_rf)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dt_feature_importances = pd.DataFrame({'Feature': indices, 'Importance_DT': importances[indices]})\n",
    "rf_feature_importances = pd.DataFrame({'Feature': indices_rf, 'Importance_RF': importances_rf[indices_rf]})\n",
    "\n",
    "merged_feature_importances = pd.merge(dt_feature_importances, rf_feature_importances, on='Feature')\n",
    "\n",
    "print(\"Merged Feature Importances:\")\n",
    "print(merged_feature_importances)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"percentile_threshold = 0.8\n",
    "\n",
    "importance_threshold_dt = merged_feature_importances['Importance_DT'].quantile(percentile_threshold)\n",
    "importance_threshold_rf = merged_feature_importances['Importance_RF'].quantile(percentile_threshold)\n",
    "\n",
    "print(\"Threshold value based on the top\", int(percentile_threshold * 100), \"percentileDT:\", importance_threshold_dt, \"percentileRF:\", importance_threshold_rf)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"merged_feature_importances = merged_feature_importances[(merged_feature_importances['Importance_DT'] > importance_threshold_dt) & (merged_feature_importances['Importance_RF'] > importance_threshold_rf)]\n",
    "print(\"Merged Feature Importances:\")\n",
    "print(merged_feature_importances)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping the feature which are not in the percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X = X.drop(X.columns.difference(X.columns[merged_feature_importances['Feature']]), axis=1)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"len(X.columns)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA train test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell building - Random forest and AdaBoost with Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 80 percentil dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "base_rf = RandomForestClassifier(n_jobs=-1, n_estimators=150, max_depth=12, random_state=42, criterion='entropy', max_features='log2', oob_score=True, verbose=1)\n",
    "base_ada = AdaBoostClassifier(n_estimators=150, random_state=42, learning_rate=1.5)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('rf', base_rf), ('ada', base_ada)], voting='soft', verbose=True)\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "\n",
    "accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
    "print(\"Accuracy (Voting Classifier):\", accuracy_voting)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "base_rf = RandomForestClassifier(n_jobs=-1, n_estimators=150, max_depth=12, random_state=42, criterion='entropy', max_features='log2', oob_score=True, verbose=1)\n",
    "base_ada = AdaBoostClassifier(n_estimators=150, random_state=42, learning_rate=1.5)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('rf', base_rf), ('ada', base_ada)], voting='soft', verbose=True)\n",
    "\n",
    "voting_clf.fit(X_train_pca, y_train_pca)\n",
    "\n",
    "y_pred_voting = voting_clf.predict(X_test_pca)\n",
    "\n",
    "accuracy_voting = accuracy_score(y_test_pca, y_pred_voting)\n",
    "print(\"Accuracy (Voting Classifier):\", accuracy_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verseny_public_test = pd.read_csv('data/verseny_public_test.csv', sep=',', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 80 percentil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X_test_test = df_verseny_public_test.drop(['cookie_id'], axis=1)\n",
    "\n",
    "X_test_test = X_test_test.drop(X_test_test.columns.difference(X.columns), axis=1)\n",
    "\n",
    "y_pred_rf = voting_clf.predict_proba(X_test_test)[:, 1]\n",
    "\n",
    "df_verseny_public_test['target'] = y_pred_rf\n",
    "\n",
    "df_verseny_public_test = df_verseny_public_test[['cookie_id', 'target']]\n",
    "\n",
    "df_verseny_public_test.to_csv('data/prediction_random_forest_w_adaboost_voting_80_param.csv', index=False)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_test_test = df_verseny_public_test.drop(['cookie_id'], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_test_test)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "y_pred_rf = voting_clf.predict_proba(X_pca)[:, 1]\n",
    "\n",
    "df_verseny_public_test['target'] = y_pred_rf\n",
    "\n",
    "df_verseny_public_test = df_verseny_public_test[['cookie_id', 'target']]\n",
    "\n",
    "df_verseny_public_test.to_csv('data/prediction_random_forest_w_adaboost_voting_PCA.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
